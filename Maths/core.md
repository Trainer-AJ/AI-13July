## Calculas
- Calculus Math is generally used in Mathematical models to obtain optimal solutions. It helps us to understand the changes between the values which are related by a function
- To get the optimal solution, derivatives are used to find the maxima and minima values of a function.
> The derivative is the measure of the rate of change (the tangent or slope) of a function whereas integral is the measure of the area under the curve.
- **Partial derivative**: It tells you: How the function changes with respect to one variable, while keeping all other variables constant.
### üß† Intuition:
Imagine a **hilly landscape** (a 3D surface) where:
- \( x \) is east-west,
- \( y \) is north-south,
- and the height \( z = f(x, y) \) is the elevation.

A **partial derivative** answers:
- ‚ÄúIf I walk only east-west (change \( x \)), how steep is the hill?‚Äù
- That‚Äôs \( \frac{\partial f}{\partial x} \)

---

### üìå Example:
Let‚Äôs say:

$$
f(x, y) = x^2 + 3xy + y^2
$$

Then:
- \( \frac{\partial f}{\partial x} = 2x + 3y \)
- \( \frac{\partial f}{\partial y} = 3x + 2y \)

Each tells you how the function changes **in one direction**, holding the other variable fixed.


---

 > gradient function just helps us getting the slope of any line in a multi dimensional space.It always points where the line is increasing (works like a compass)
Yes ‚Äî you‚Äôve got the **core idea exactly right**!

---

### ‚úÖ Summary:
The **gradient function** helps us find the **slope** (rate of change) of a function in **multi-dimensional space** ‚Äî where the function depends on multiple variables like \( x, y, z, \dots, n \).

---

### üîç Why Gradient Is Powerful:
- In **1D**, a derivative tells you the slope of a curve.
- In **multi-D**, the **gradient** gives you a **vector** that points in the direction of **steepest increase** of the function.
- It combines all the **partial derivatives** into one vector:
  $$
  \nabla f(x, y, z, \dots) = \left[ \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}, \dots \right]
  $$

---

### üß≠ Use Case:
In optimization (like training a machine learning model), the gradient tells us:
- **Which direction to move** in parameter space
- **How fast to move** (based on the magnitude of the gradient)
- So we can **minimize the loss function**

